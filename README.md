# VAPID
The **V**isual and **A**uditory **P**rocessed **I**dentification **D**ataset (VAPID) is a human-curated shared task collection of audiovisual media intended for computational research in natural language processing (NLP) to create models for domains such as automatic speech recognition, speech translation, audio description, and speaker verification or diarization.

Currently, to most AI models, this audiovisual media is **vapid**—or perceived as unstimulating—because they are hardly trained on videos accompanied by audio, and we are trying to change that.
